{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from itertools import islice\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, n=2):\n",
        "        self.n = n\n",
        "        self.ngram_counts = Counter()\n",
        "        self.context_counts = Counter()\n",
        "        self.vocab = set()\n",
        "        self.total_words = 0\n",
        "\n",
        "    def words(self, text):\n",
        "        \"\"\" Breaks the text into words and makes them lowercase \"\"\"\n",
        "        return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def generate_ngrams(self, words_list):\n",
        "        \"\"\" Generating N-grams from a list of words \"\"\"\n",
        "        return zip(*[islice(words_list, i, None) for i in range(self.n)])\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\" Model training \"\"\"\n",
        "        for sentence in corpus:\n",
        "            tokens = self.words(\" \".join(sentence))\n",
        "            self.vocab.update(tokens)\n",
        "            self.total_words += len(tokens)\n",
        "\n",
        "            for ngram in self.generate_ngrams(tokens):\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.context_counts[ngram[:-1]] += 1\n",
        "\n",
        "    def get_prob(self, context, word):\n",
        "        \"\"\" Returns probability P(word | context) \"\"\"\n",
        "        context = tuple(context)\n",
        "        ngram = context + (word,)\n",
        "\n",
        "        count_ngram = self.ngram_counts.get(ngram, 0)\n",
        "        count_context = self.context_counts.get(context, 0)\n",
        "\n",
        "        if count_context == 0:\n",
        "            return 1e-6 \n",
        "        return count_ngram / count_context\n",
        "\n",
        "    def get_log_prob(self, context, word):\n",
        "        \"\"\" Logarithmic probability P(word | context) \"\"\"\n",
        "        prob = self.get_prob(context, word)\n",
        "        return math.log(prob) if prob > 0 else -float('inf')\n",
        "\n",
        "    def predict_next_word(self, context):\n",
        "        \"\"\" Predicts the next word based on probabilities \"\"\"\n",
        "        context = tuple(context)\n",
        "        candidates = {word: self.get_prob(context, word) for word in self.vocab}\n",
        "        return max(candidates, key=candidates.get) if candidates else None\n",
        "\n",
        "\n",
        "def words(text):\n",
        "    \"\"\" Breaks the text into words and makes them lowercase \"\"\"\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def load_corpus(filename):\n",
        "    \"\"\" Text file downloading \"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return words(f.read())\n",
        "\n",
        "def build_ngram_model(corpus_file, n=2):\n",
        "    \"\"\" Creating N-gram model \"\"\"\n",
        "    model = NGramModel(n)\n",
        "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "        sentences = [line.strip().split() for line in f.readlines()]\n",
        "    model.train(sentences)\n",
        "    return model\n",
        "\n",
        "bigram_model = build_ngram_model('/Users/renat/Documents/study/nlp/nlp-assignment-1/dataset/big.txt', 2)\n",
        "trigram_model = build_ngram_model('/Users/renat/Documents/study/nlp/nlp-assignment-1/dataset/big.txt', 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Words correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrected text: king sport is very important\n"
          ]
        }
      ],
      "source": [
        "def known(words):\n",
        "    \"\"\" Filters words that exist in the corpus \"\"\"\n",
        "    return set(w for w in words if w in bigram_model.vocab)\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    dp = [[0 for j in range(len(s2)+1)] for i in range(len(s1)+1)]\n",
        "    for i in range(1, len(s2)+1):\n",
        "        dp[0][i] = i\n",
        "    \n",
        "    for i in range(1, len(s1)+1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            dp[i][j] = min(min(dp[i-1][j], dp[i][j-1]) + 1, dp[i-1][j-1] + (1 if s1[i-1] != s2[j-1] else 0))\n",
        "    \n",
        "    return dp[len(s1)][len(s2)]\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\" All words that are one edit away from the current word \"\"\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits  = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts  = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"\"\" All words that are two edits away from the current word \"\"\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def edits3(word):\n",
        "    \"\"\" All words that are three edits away from the current word \"\"\"\n",
        "    return set((e2 for e1 in edits2(word) for e2 in edits2(e1)))\n",
        "\n",
        "def best_correction(word, prev_word=None, next_word=None):\n",
        "    \"\"\" Selects the best correction using both edit distance and N-gram probability \"\"\"\n",
        "    possible_corrections = known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
        "\n",
        "    if not possible_corrections:\n",
        "        return word\n",
        "\n",
        "    def score(w):\n",
        "        edit_dist = edit_distance(word, w)\n",
        "        ngram_prob = (\n",
        "            trigram_model.get_log_prob((prev_word,), w) if prev_word else\n",
        "            bigram_model.get_log_prob((\"\",), w)\n",
        "        )\n",
        "        return ngram_prob - 2 * edit_dist\n",
        "\n",
        "    best_word = max(possible_corrections, key=score)\n",
        "\n",
        "    return best_word\n",
        "\n",
        "\n",
        "def correct_text(sentence):\n",
        "    \"\"\" Corrects spelling errors in the text considering the context \"\"\"\n",
        "    words_list = words(sentence)\n",
        "    corrected_words = []\n",
        "    \n",
        "    for i, word in enumerate(words_list):\n",
        "        prev_word = words_list[i - 1] if i > 0 else None\n",
        "        next_word = words_list[i + 1] if i < len(words_list) - 1 else None\n",
        "        corrected_words.append(best_correction(word, prev_word, next_word))\n",
        "    \n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "test_sentence = \"dking sport is very important\"\n",
        "corrected = correct_text(test_sentence)\n",
        "print(\"Corrected text:\", corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "#### Choice of N-gram Dataset\n",
        "I use big.txt as the dataset because:\n",
        "\n",
        "- It is a large, real-world corpus containing words from literature, making it suitable for estimating word probabilities.\n",
        "- It provides a diverse vocabulary, reducing the risk of unseen words.\n",
        "\n",
        "But it still was not enough, for example \"dking species\" is \"doing species\", because this phrase is not known. Alternatives like Wikipedia dumps or modern news articles could be used.\n",
        "\n",
        "Also, I have had a problem while downloading files from https://www.ngrams.info/download_coca.asp, the site didn't give me the access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Choice of N-gram Model\n",
        "\n",
        "I implemented both bigram and trigram models:\n",
        "\n",
        "- Bigram model (n=2) captures basic word dependencies (e.g., \"good morning\" is more likely than \"good turtle\").\n",
        "- Trigram model (n=3) improves contextual understanding by considering two prior words, useful for disambiguation (e.g., \"New York city\" vs. \"New York times\").\n",
        "\n",
        "Using only unigrams would be too simplistic, while higher N-grams (4,5+) require significantly more data and memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Handling Unknown Words & Probabilities\n",
        "\n",
        "I address out-of-vocabulary (OOV) words and missing N-grams with:\n",
        "\n",
        "- Smoothing (1e-6 probability for unseen words): Ensures no word gets a probability of zero, preventing computational errors.\n",
        "- Backoff Strategy: If a trigram is missing, we rely on a bigram; if the bigram is missing, we fall back to unigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Edit Distance & Word Correction Strategy\n",
        "\n",
        "To correct spelling mistakes, I prioritize words based on:\n",
        "- Existence in the vocabulary (if known, it’s likely correct).\n",
        "- Single-edit distance (edit1()) words are preferred, as they are more likely to be the intended word.\n",
        "- Double-edit distance (edit2()) is used only if no single-edit candidates exist.\n",
        "- Triple-edit distance edit3()\n",
        "- Trigram/Bigram Probability: Among candidates, I select the word that maximizes P(word | context).\n",
        "\n",
        "In future, i could also implement dynamic weight assignment for edits, but I assume a uniform penalty for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N-gram Corrector Accuracy (20% noise): 0.95\n",
            "N-gram Corrector Accuracy (40% noise): 0.80\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "import re\n",
        "\n",
        "def introduce_errors(sentence, noise_prob=0.2):\n",
        "    \"\"\" Introduces spelling errors into a sentence with given probability \"\"\"\n",
        "    words_list = sentence.split()\n",
        "    noisy_sentence = []\n",
        "    \n",
        "    for word in words_list:\n",
        "        if random.random() < noise_prob:\n",
        "            typo_candidates = list(edits1(word))\n",
        "            if typo_candidates:\n",
        "                word = random.choice(typo_candidates)\n",
        "        noisy_sentence.append(word)\n",
        "    \n",
        "    return \" \".join(noisy_sentence)\n",
        "\n",
        "def generate_test_set(clean_sentences, noise_prob=0.2):\n",
        "    \"\"\" Generates a test set with noise added \"\"\"\n",
        "    return [introduce_errors(sentence, noise_prob) for sentence in clean_sentences]\n",
        "\n",
        "def evaluate_corrector(corrector, test_sentences, ground_truth):\n",
        "    \"\"\" Evaluates the spelling corrector's accuracy at the word level \"\"\"\n",
        "    correct_word_count = 0\n",
        "    total_word_count = 0\n",
        "\n",
        "    for i in range(len(test_sentences)):\n",
        "        corrected_sentence = corrector(test_sentences[i]).split()\n",
        "        ground_truth_sentence = ground_truth[i].split()\n",
        "\n",
        "        min_length = min(len(corrected_sentence), len(ground_truth_sentence))\n",
        "\n",
        "        for j in range(min_length):\n",
        "            if corrected_sentence[j] == ground_truth_sentence[j]:\n",
        "                correct_word_count += 1\n",
        "            total_word_count += 1\n",
        "\n",
        "    accuracy = correct_word_count / total_word_count if total_word_count > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def extract_sentences(filename, min_length=5, num_sentences=10):\n",
        "    \"\"\" Extracts random sentences from the given text file. \"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]', text) if len(s.strip().split()) >= min_length]\n",
        "    \n",
        "    return random.sample(sentences, num_sentences)\n",
        "\n",
        "\n",
        "holmes_sentences = [\n",
        "    re.sub(r'[^a-zA-Z0-9\\s]', '', s.lower()) \n",
        "    for s in extract_sentences('/Users/renat/Documents/study/nlp/nlp-assignment-1/dataset/big.txt', num_sentences=10)\n",
        "]\n",
        "\n",
        "test_set_20 = generate_test_set(holmes_sentences, noise_prob=0.2)\n",
        "test_set_40 = generate_test_set(holmes_sentences, noise_prob=0.4)\n",
        "\n",
        "accuracy_ngram_20 = evaluate_corrector(correct_text, test_set_20, holmes_sentences)\n",
        "accuracy_ngram_40 = evaluate_corrector(correct_text, test_set_40, holmes_sentences)\n",
        "\n",
        "print(f\"N-gram Corrector Accuracy (20% noise): {accuracy_ngram_20:.2f}\")\n",
        "print(f\"N-gram Corrector Accuracy (40% noise): {accuracy_ngram_40:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
